doc <- paste(doc, collapse=" ")
# extract sentences
doc <- ParseSentences(doc = doc)
dtm <- MakeSentenceDtm(doc = doc)
dim(dtm)
summary(rowSums(dtm))
hist(rowSums(dtm))
hist(rowSums(dtm), breaks=100)
Which(rowSums(dtm) > 50 )
shich(rowSums(dtm) > 50 )
which(rowSums(dtm) > 50 )
doc[ 1 ]
doc[ 11 ]
doc[ 117 ]
dtm <- dtm[ rowSums(dtm) %in% 5:20, ] # keep only sentences that have at least 5 words and less than 21
g <- MakeSentenceAdjmat(dtm = dtm)
top.4 <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 4)
summary <- paste(top.4, collapse=" ")
write.table(summary, file = "output/summary.txt", row.names=FALSE, col.names=FALSE, sep="\n")
source('~/.active-rstudio-document', echo=TRUE)
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 10)
# paste together for final result and write to file
summary <- paste(top.n, collapse=" ")
write.table(summary, file = "output/summary.txt", row.names=FALSE, col.names=FALSE, sep="\n")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 7)
# paste together for final result and write to file
summary <- paste(top.n, collapse=" ")
write.table(summary, file = "output/summary.txt", row.names=FALSE, col.names=FALSE, sep="\n")
source("scripts/SummaryFunctions.R")
rm(list=ls())
source("scripts/SummaryFunctions.R")
files <- grep("\.txt$", dir("//div-stpi/public/Tommy/NSTC/Pdfs/"), value = TRUE)
files <- grep("\\.txt$", dir("//div-stpi/public/Tommy/NSTC/Pdfs/"), value = TRUE)
filepath <- "//div-stpi/public/Tommy/NSTC/Pdfs/"
files <- grep("\\.txt$", dir(filepath), value = TRUE)
library(snowfall)
rm(list=ls())
source("scripts/SummaryFunctions.R")
filepath <- "//div-stpi/public/Tommy/NSTC/Pdfs/"
files <- grep("\\.txt$", dir(filepath), value = TRUE)
names(files) <- files
library(snowfall)
sfInit(parallel=TRUE, cpus = 8)
sfExport("filepath")
sfSource("scripts/SummaryFunctions.R")
my.summaries <- sfSapply(files, function(FILE){
result <- SummaryFunction(path.to.file = paste(filepath, FILE, sep=""), N = 7)
return(result)
})
sfStop()
my.summaries[[ 1 ]]
names(my.summaries)
summaries <- data.frame(file=names(my.summaries), summary=unlist(my.summaries), stringsAsFactors=FALSE)
View(summaries)
library(snowfall)
sfInit(parallel=TRUE, cpus = 8)
sfExport("filepath")
sfSource("scripts/SummaryFunctions.R")
my.summaries <- sfSapply(files, function(FILE){
result <- SummaryFunction(path.to.file = paste(filepath, FILE, sep=""), N = 5)
return(result)
})
sfStop()
summaries <- data.frame(file=names(my.summaries), summary=unlist(my.summaries), stringsAsFactors=FALSE)
View(summaries)
write.table("output/NSTC_Summaries.txt", sep=" | ", row.names=FALSE)
write.table(summaries, "output/NSTC_Summaries.txt", sep=" | ", row.names=FALSE)
write.table(summaries, "output/NSTC_Summaries.txt", sep="|", row.names=FALSE)
library(Matrix)
sparseVector(as.numeric(c(TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE)))
as.sparseVector(as.numeric(c(TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE)))
sparseVector(c(TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE))
sparseVector(c(TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE), length=8)
x <- c(TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE)
y = ! x
blah <- list(x=x, y=y)
blah <- lapply(blah, as.numeric)
blah <- lapply(blah, function(vec) Matrix(vec, nrow=1, sparse=T))
unlist(blah)
blerg <- do.call(rBind, blah)
blerg
blerg %*% blerg
blerg %*% c(blerg)
blerg %*% t(blerg)
blerg1 <- blerg > 0
blerg1
blerg2 <- blerg1 %*% t(blerg1)
blerg2
library(igraph)
install.packages(igraph)
install.packages("igraph")
library(igraph)
help(evcent)
source('~/Documents/DocSummarizer/scripts/SummaryFunctions.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
rm(list=ls())
path <- "http://www2.warwick.ac.uk/fac/soc/al/research/collect/base/lecturetranscripts/ss/"
files <- scan(path, what="character", sep="\n")
files <- grep("_nopause\\.txt", files, value=TRUE)
files
files <- strsplit(files, split="<a href=\"|\">")
files[[1]]
files[[2]]
files <- sapply(files, function(x) grep("http:", x, value=TRUE))
rile
files
warwick.files <- sapply(files, function(x){
doc <- scan(x, what="character", sep="\n")
doc <- paste(doc, collapse=" ")
return(doc)
})
save(warwick.files, file="data/Warwick.SocialScienceLectures.RData")
View(warwick.files)
data(Reuters)
data(reuters)
data(nyt)
mydocs <- grep("Wikipedia", dir("data/"), value=TRUE)
mydocs <- sapply(mydocs, function(DOC){
result <- scan(paste("data", DOC, sep="/"), sep="\n", what="character")
return(result)
})
rm(list=ls())
mydocs <- grep("Wikipedia", dir("data/"), value=TRUE)
mydocs <- sapply(mydocs, function(DOC){
result <- scan(paste("data", DOC, sep="/"), sep="\n", what="character")
result <- paste(result, collapse=" ")
return(result)
})
source('~/Documents/DocSummarizer/scripts/SummaryFunctions.R', echo=TRUE)
mydtms <- lapply(mydocs, function(doc){
# extract sentences
doc <- ParseSentences(doc = doc)
# get dtm of sentences
dtm <- MakeSentenceDtm(doc = doc)
return(list(doc=doc, dtm=dtm))
})
mysentences <- lapply(mydocs, function(doc){
# extract sentences
doc <- ParseSentences(doc = doc)
return(doc)
})
View(mysentences[[ 1 ]])
View(mysentences[[ 2 ]])
View(mysentences[[ 3 ]])
mydtms <- lapply(mysentences, function(doc){
# get dtm of sentences
dtm <- MakeSentenceDtm(doc = doc)
return(dtm)
})
for(j in 1:3){}
for(j in 1:3){
blah <- MakeSentenceDtm(doc=mysentences[[ j ]])
}
for(j in 2:3){
blah <- MakeSentenceDtm(doc=mysentences[[ j ]])
}
doc <- mysentences[[ 1 ]]
doc <- gsub("[^a-zA-Z]", " ", doc) # removes all non-alphabetic characters
doc <- tolower(doc) #lowercase
doc <- gsub(" +", " ", doc) # removes extra spaces
corp <- Corpus(VectorSource(doc))
corp <- tm_map(corp, removeWords, gsub("[^a-zA-Z]", " ", c(stopwords("english"), stopwords("SMART")) ) ) # remove stopwords
corp <- tm_map(corp, stripWhitespace) # remove spaces again
corp <- tm_map(corp, stemDocument) # stem document
rm(list=ls())
# load data
mydocs <- grep("Wikipedia", dir("data/"), value=TRUE)
mydocs <- sapply(mydocs, function(DOC){
result <- scan(paste("data", DOC, sep="/"), sep="\n", what="character")
result <- paste(result, collapse=" ")
return(result)
})
# parse sentences
mysentences <- lapply(mydocs, function(doc){
# extract sentences
doc <- ParseSentences(doc = doc)
return(doc)
})
# get dtms
mydtms <- lapply(mysentences, function(doc){
# get dtm of sentences
dtm <- MakeSentenceDtm(doc = doc, stem=FALSE)
return(dtm)
})
source("scripts/SummaryFunctions.R")
rm(list=ls())
source("scripts/SummaryFunctions.R")
# load data
required.packages <- c("tm",
"openNLP",
"NLP",
"RWeka",
"Matrix",
"slam",
"igraph")
install <- try(required.packages[ ! required.packages %in% installed.packages() ], silent = TRUE)
class(install)
length(install)
rm(list=ls())
source("scripts/SummaryFunctions.R")
# load data
rm(list=ls())
source("scripts/SummaryFunctions.R")
# load data
mydocs <- grep("Wikipedia", dir("data/"), value=TRUE)
mydocs <- sapply(mydocs, function(DOC){
result <- scan(paste("data", DOC, sep="/"), sep="\n", what="character")
result <- paste(result, collapse=" ")
return(result)
})
# parse sentences
mysentences <- lapply(mydocs, function(doc){
# extract sentences
doc <- ParseSentences(doc = doc)
return(doc)
})
# get dtms
mydtms <- lapply(mysentences, function(doc){
# get dtm of sentences
dtm <- MakeSentenceDtm(doc = doc, stem=FALSE)
return(dtm)
})
mykeywords <- lapply(mydtms, function(x){
ExtractKeywords(dtm=x, M=7)
})
View(do.call(rbind, mykeywords))
mykeywords[[ 1 ]]
dtm <- mydtms[[ 1 ]]
dtm <- dtm > 0 # becomes a matrix of ones and zeros, basically "has word" or does not
source('~/Documents/DocSummarizer/data/ExampleSummary.R', echo=TRUE)
mykeywords[[ 1 ]]
View(do.call(rbind, mykeywords))
summaries.raw <- lapply(mydtms, function(dtm){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="raw")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = N, method=method)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
})
source("scripts/SummaryFunctions.R")
summaries.raw <- lapply(mydtms, function(dtm){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="raw")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = N, method=method)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
})
summaries.raw <- lapply(mydtms, function(dtm){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="raw")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = N)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
})
summaries.raw <- lapply(mydtms, function(dtm){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="raw")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
})
summaries.raw <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="raw")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mydocs)
summaries.csim <- mapply(function(dtm){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="cosine")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mydocs)
summaries.key <- mapply(function(dtm){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="keyword")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mydocs)
summaries.csim <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="cosine")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mydocs)
##########################################
# Summarize based on keywords
##########################################
summaries.key <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="keyword")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mydocs)
source('~/Documents/DocSummarizer/scripts/SummaryFunctions.R', echo=TRUE)
summaries.key <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="keyword")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mydocs)
View(do.call(rbind, summaries.key))
View(summaries.key)
View(summaries.csim)
View(summaries.raw)
dtm <- mydtms[[ 1 ]]
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
dim(dtm)
doc <- mydocs[[ 1 ]]
g <- MakeSentenceAdjmat(dtm = dtm, method="raw")
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
summary <- paste(top.n, collapse=" ")
summary
top.n
rownames(dtm)
names(g)
V(g)$name
blah <- evcent(g)
blah <- names(blah$vector)[ order(blah$vector) ][ 1:5 ]
blah
doc[ blah ]
names(doc)
summaries.raw <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="raw")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mysentences)
summaries.raw
View(summaries.raw)
##########################################
# Summarize based on cosine similarity
##########################################
summaries.csim <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="cosine")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mysentences)
##########################################
# Summarize based on keywords
##########################################
summaries.key <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="keyword")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mysentences)
View(summaries.csim)
View(summaries.key)
summaries.key == summaries.raw
View(summaries.raw)
library(RWeka)
source('~/Documents/DocSummarizer/scripts/SummaryFunctions.R', echo=TRUE)
rm(list=ls())
source("scripts/SummaryFunctions.R")
# load data
mydocs <- grep("Wikipedia", dir("data/"), value=TRUE)
mydocs <- sapply(mydocs, function(DOC){
result <- scan(paste("data", DOC, sep="/"), sep="\n", what="character")
result <- paste(result, collapse=" ")
return(result)
})
# parse sentences
mysentences <- lapply(mydocs, function(doc){
# extract sentences
doc <- ParseSentences(doc = doc)
return(doc)
})
mydtms <- lapply(mysentences, function(doc){
# get dtm of sentences
dtm <- MakeSentenceDtm(doc = doc, stem=FALSE, min.ngram=2, max.ngram=3)
return(dtm)
})
mydtms <- lapply(mysentences, function(doc){
# get dtm of sentences
dtm <- MakeSentenceDtm(doc = doc, stem=FALSE, min.ngram=1, max.ngram=1)
return(dtm)
})
tokenize <- NgramTokenizer(min=min.ngram, max=max.ngram)
source('~/Documents/DocSummarizer/scripts/SummaryFunctions.R', echo=TRUE)
mydtms <- lapply(mysentences, function(doc){
# get dtm of sentences
dtm <- MakeSentenceDtm(doc = doc, stem=FALSE, min.ngram=1, max.ngram=1)
return(dtm)
})
doc <- mysentences[[ 1 ]]
doc <- gsub("[^a-zA-Z]", " ", doc) # removes all non-alphabetic characters
doc <- tolower(doc) #lowercase
doc <- gsub(" +", " ", doc) # removes extra spaces
corp <- Corpus(VectorSource(doc))
corp <- tm_map(corp, removeWords, gsub("[^a-zA-Z]", " ", c(stopwords("english"), stopwords("SMART")) ) ) # remove stopwords
corp <- tm_map(corp, stripWhitespace) # remove spaces again
tokenize <- NgramTokenizer(min=min.ngram, max=max.ngram)
dtm <- DocumentTermMatrix(corp, control=list(tokenize = tokenize))
source('~/Documents/DocSummarizer/scripts/SummaryFunctions.R', echo=TRUE)
mydtms <- lapply(mysentences, function(doc){
# get dtm of sentences
dtm <- MakeSentenceDtm(doc = doc, stem=FALSE, min.ngram=1, max.ngram=1)
return(dtm)
})
mykeywords <- lapply(mydtms, function(x){
ExtractKeywords(dtm=x, M=7)
})
##########################################
# Summarize based on "raw" word counts
##########################################
summaries.raw <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="raw")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mysentences)
##########################################
# Summarize based on cosine similarity
##########################################
summaries.csim <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="cosine")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mysentences)
##########################################
# Summarize based on keywords
##########################################
summaries.key <- mapply(function(dtm, doc){
# keep only sentences that have at least 5 words and less than 21
dtm <- dtm[ rowSums(dtm) %in% 5:20, ]
# get adjacency matrix
g <- MakeSentenceAdjmat(dtm = dtm, method="keyword")
# top N sentences based on eigenvector centrality
top.n <- SentenceEigenRank(igraph.object = g, sentences = doc, N = 5)
# paste together for final result and output
summary <- paste(top.n, collapse=" ")
return(summary)
}, dtm=mydtms, doc=mysentences)
View(summaries.csim)
summaries.csim[[ 1 ]]
source('~/Documents/DocSummarizer/scripts/ExampleSummary.R', echo=TRUE)
